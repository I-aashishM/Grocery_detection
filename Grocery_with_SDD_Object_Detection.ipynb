{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grocery_with_SDD_Object_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt-69GJgnAFp",
        "colab_type": "text"
      },
      "source": [
        "This notebook is used in Google Colab for accessing free GPU in order to detect the number of product in given grocery shelves image.\n",
        "\n",
        "The dataset is provided by Varol Gul and Kuzu, Ridvan S. for the title \"Toward Retail Product Recognition on Grocery Shelves\" in 2014. It contains 354 tobacco shelves images collected from ~40 locations with 4 cameras.\n",
        "\n",
        "Dataset can be download for [here](https://github.com/gulvarol/grocerydataset)\n",
        "\n",
        "Notebook contains:\n",
        "\n",
        "\n",
        "1.   Installation of Tensorflow object detection API in Google Colab\n",
        "2.   Data Preparation\n",
        "3.   Training\n",
        "4.   Inference\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9ctwjZTh1Rq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x  ### we will be using Tensorflow 1.x because Tensorflow object detection API does not support training in Tensorflow 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVeXdzgotcWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone --quiet https://github.com/tensorflow/models.git ## Git command to download the Tensorflow Onject detection Model\n",
        "\n",
        "## Prerequisite for the Tensorflow Onject detection Model\n",
        "!apt-get install -qq protobuf-compiler python-tk  \n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib PyDrive\n",
        "!pip install -q pycocotools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob8k81Q-tiIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.  ## Bash command to create .py files in proto folder."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c4LmDjFtk3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Testing the Tensorflow Object Detection model, if it occurs any error there is something wrong.\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n",
        "\n",
        "!python /content/models/research/object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cjJ8jLItlR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash \n",
        "cd models/research\n",
        "pip install .  ## installing the object-detection module"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypHqI25ntnDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##importing libraries\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "from object_detection.utils import dataset_util"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0WOTkOFtpKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###Downloading the Dataset\n",
        "%cd /content\n",
        "! wget https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part1.tar.gz\n",
        "! wget https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part2.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaSNpYqdtrU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Creating folder to store downloaded datasets\n",
        "! mkdir 'data'\n",
        "! mkdir 'data/images'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArmRH46ittUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Extracting the downloaded daaset in \"data/images\" folder.\n",
        "!tar -xvf  'GroceryDataset_part1.tar.gz'  -C 'data/images'\n",
        "!tar -xvf  'GroceryDataset_part2.tar.gz'  -C 'data/images'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aVtunlmtvD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## importing libraries\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from collections import namedtuple, OrderedDict\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fryiIGn8rtXR",
        "colab_type": "text"
      },
      "source": [
        "Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POtSc-4DtyaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Path\n",
        "data_path = 'data/'\n",
        "shelf_images = 'data/images/ShelfImages/'\n",
        "shelf_images_train = 'data/images/ShelfImages1/ShelfImages/train'\n",
        "shelf_images_test = 'data/images/ShelfImages1/ShelfImages/test'\n",
        "product_images = 'data/images/ProductImagesFromShelves/'\n",
        "cropped_path_train = '/content/data/images/ShelfImages1/train'\n",
        "cropped_path_test = '/content/data/images/ShelfImages1/eval'\n",
        "cropped_path = 'data/cropped/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EUOrAYRt0nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Creating dataframe of all the shelf images\n",
        "jpg_files = [f for f in os.listdir(f'{shelf_images}') if f.endswith('JPG')]\n",
        "photos_df = pd.DataFrame([[f, f[:6], f[7:14]] for f in jpg_files], \n",
        "                         columns=['file', 'shelf_id', 'planogram_id'])\n",
        "photos_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFHgiWbYt7fN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Creating dataframe of all the product images\n",
        "products_df = pd.DataFrame(\n",
        "    [[f[:18], f[:6], f[7:14], i, *map(int, f[19:-4].split('_'))] \n",
        "     for i in range(11) \n",
        "     for f in os.listdir(f'{product_images}{i}') if f.endswith('png')],\n",
        "    columns=['file', 'shelf_id', 'planogram_id', \n",
        "             'category', 'xmin', 'ymin', 'w', 'h'])\n",
        "# convert from width height to xmax, ymax\n",
        "products_df['xmax'] = products_df['xmin'] + products_df['w']\n",
        "products_df['ymax'] = products_df['ymin'] + products_df['h']\n",
        "products_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRhphqmPuBbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get distinct shelves\n",
        "shelves = list(set(photos_df['shelf_id'].values))\n",
        "# use train_test_split from sklearn\n",
        "shelves_train, shelves_validation, _, _ = train_test_split(\n",
        "    shelves, shelves, test_size=0.3, random_state=6)\n",
        "# mark all records in data frames with is_train flag\n",
        "def is_train(shelf_id): return shelf_id in shelves_train\n",
        "photos_df['is_train'] = photos_df.shelf_id.apply(is_train)\n",
        "products_df['is_train'] = products_df.shelf_id.apply(is_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXxu5J3puE9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = products_df[products_df.category != 0].\\\n",
        "         groupby(['category', 'is_train'])['category'].\\\n",
        "         count().unstack('is_train').fillna(0)\n",
        "df.plot(kind='barh', stacked=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZDVUmzouH0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "photos_df.to_pickle(f'{data_path}photos.pkl')\n",
        "products_df.to_pickle(f'{data_path}products.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR0-vJJCuLKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "photos = pd.read_pickle(f'{data_path}photos.pkl')\n",
        "products = pd.read_pickle(f'{data_path}products.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wUEzcl5r1Ny",
        "colab_type": "text"
      },
      "source": [
        "Cropping 6 random squares from each image and resize crops to 600x600 pictures with same aspect ratio\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEFDG2iIufLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_CROP_TRIALS = 6\n",
        "CROP_SIZE = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQlT1p4kufx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# returns random value in [s, f]\n",
        "def rand_between(s, f):\n",
        "    if s == f:\n",
        "        return s\n",
        "    return np.random.randint(s, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWUD6lc1uiCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_products, eval_products = [], []\n",
        "for img_file, is_train in photos[['file', 'is_train']].values:\n",
        "    img = cv2.imread(f'{shelf_images}{img_file}')\n",
        "    img_h, img_w, img_c = img.shape\n",
        "    for n in range(N_CROP_TRIALS):\n",
        "        # randomly crop square\n",
        "        c_size = rand_between(300, max(img_h, img_w))\n",
        "        x0 = rand_between(0, max(0, img_w - c_size))\n",
        "        y0 = rand_between(0, max(0, img_h - c_size))\n",
        "        x1 = min(img_w, x0 + c_size)\n",
        "        y1 = min(img_h, y0 + c_size)\n",
        "        # products totally inside crop rectangle\n",
        "        crop_products = products[(products.file == img_file) & \n",
        "                                 (products.xmin > x0) & (products.xmax < x1) &\n",
        "                                 (products.ymin > y0) & (products.ymax < y1)]\n",
        "        # no products inside crop rectangle? cropping trial failed...\n",
        "        if len(crop_products) == 0:\n",
        "            continue\n",
        "        # name the crop\n",
        "        crop_img_file = f'{img_file[:-4]}{x0}_{y0}_{x1}_{y1}.JPG'\n",
        "        # crop and reshape to CROP_SIZExCROP_SIZE or smaller \n",
        "        # keeping aspect ratio\n",
        "        crop = img[y0:y1, x0:x1]\n",
        "        h, w, c = crop.shape\n",
        "        ratio = min(CROP_SIZE/h, CROP_SIZE/w)\n",
        "        crop = cv2.resize(crop, (0,0), fx=ratio, fy=ratio)\n",
        "        crop = crop[0:CROP_SIZE, 0:CROP_SIZE]\n",
        "        h, w, c = crop.shape\n",
        "        # add crop inner products to train_products or eval_products list\n",
        "        for xmin, ymin, xmax, ymax in \\\n",
        "                crop_products[['xmin', 'ymin', 'xmax', 'ymax']].values:\n",
        "            xmin -= x0\n",
        "            xmax -= x0\n",
        "            ymin -= y0\n",
        "            ymax -= y0\n",
        "\n",
        "            xmin, xmax, ymin, ymax = [int(np.round(e * ratio)) \n",
        "                                      for e in [xmin, xmax, ymin, ymax]]\n",
        "            product = {'filename': crop_img_file, 'class':'pack', \n",
        "                       'width':w, 'height':h,\n",
        "                       'xmin':xmin, 'ymin':ymin, 'xmax':xmax, 'ymax':ymax}\n",
        "            if is_train:\n",
        "                train_products.append(product)\n",
        "            else:\n",
        "                eval_products.append(product)\n",
        "        # save crop top eval or train folder\n",
        "        subpath = ['eval/', 'train/'][is_train]\n",
        "        cv2.imwrite(f'{cropped_path}{subpath}{crop_img_file}', crop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gAz2TVouliV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.DataFrame(train_products).set_index('filename')\n",
        "eval_df = pd.DataFrame(eval_products).set_index('filename')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O6S6rV7uoBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(train_df))\n",
        "print(len(eval_df))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zxB3dL6wDmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Creating tf.record for training and testing\n",
        "\n",
        "def class_text_to_int(row_label):\n",
        "    if row_label == 'pack':\n",
        "        return 1\n",
        "    else:\n",
        "        None\n",
        "\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) \n",
        "            for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(class_text_to_int(row['class']))\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90oEK7eXxG2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_tf_records(images_path, examples, dst_file):\n",
        "    writer = tf.python_io.TFRecordWriter(dst_file)\n",
        "    grouped = split(examples, 'filename')\n",
        "    for group in grouped:\n",
        "        tf_example = create_tf_example(group, images_path)\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "    writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeT9dzXdxHdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convert_to_tf_records(f'{cropped_path}/train/', train_df, f'{data_path}train.record')\n",
        "convert_to_tf_records(f'{cropped_path}/eval/', eval_df, f'{data_path}test.record')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7A5p8bRxLFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Creating training folder and we will put all the files in it. First, we put ssd_mobilenet COCO model in it. Second, we put ssd_mobilenet_v1_coco.config\n",
        "## Third we put label_map.pbtxt in it. Lastly, We put tf.record of train and test in it.\n",
        "! mkdir '/content/models/research/object_detection/training'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMtMmsXPxXlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz ## download ssd_mobilenet COCO model, you can download whatever model for object detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dXqGIEqxZj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Extracting the model in training folder\n",
        "!tar -xvf  'ssd_mobilenet_v1_coco_2017_11_17.tar.gz'  -C '/content/models/research/object_detection/training' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL3_NKCtxbhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## moving the tf.record of train and test in training folder\n",
        "!mv /content/data/test.record /content/models/research/object_detection/training\n",
        "!mv /content/data/train.record /content/models/research/object_detection/training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIWQy-l6xeA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## coping the ssd_mobilenet_v1_coco.config file to training folder\n",
        "!cp /content/models/research/object_detection/samples/configs/ssd_mobilenet_v1_coco.config /content/models/research/object_detection/training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcGFXPiTxi9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating label_map for object detection. See, we have only one label that is \"pack\", if you have more than one label you have to change this.\n",
        "\n",
        "l_map = \"item {\\n id : 1\\n name : 'pack'\\n}\"\n",
        "myfile = open(\"label_map.pbtxt\",\"w+\")\n",
        "myfile.write(l_map)\n",
        "myfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4opOzGv9xks1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## movile label_map.pbtxt file to training folder\n",
        "!mv label_map.pbtxt /content/models/research/object_detection/training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcnPFfM7xmfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Changing path, number of classes in ssd_mobilenet_v1_coco.config file. It depend upon which model you are using for detection. I used SSD_MOBILENET COCO\n",
        "## If you are using other model, you have to change according to the use. \n",
        "import re\n",
        "\n",
        "#filename = '/content/datalab/pretrained_model/pipeline.config'\n",
        "filename = '/content/models/research/object_detection/training/ssd_mobilenet_v1_coco.config'\n",
        "with open(filename) as f:\n",
        "  s = f.read()\n",
        "with open(filename, 'w') as f:\n",
        "  s = re.sub('num_classes: 90', 'num_classes: 1',s)\n",
        "  # s = re.sub('num_examples: 8000', 'num_examples: 71',s)\n",
        "  s = re.sub('PATH_TO_BE_CONFIGURED/model.ckpt', '/content/models/research/object_detection/training/ssd_mobilenet_v1_coco_2017_11_17/model.ckpt', s)\n",
        "  s = re.sub('PATH_TO_BE_CONFIGURED/mscoco_train.record-\\?\\?\\?\\?\\?-of-00100', '/content/models/research/object_detection/training/train.record', s)\n",
        "  s = re.sub('PATH_TO_BE_CONFIGURED/mscoco_val.record-\\?\\?\\?\\?\\?-of-00010', '/content/models/research/object_detection/training/test.record', s)\n",
        "  s = re.sub('PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt', '/content/models/research/object_detection/training/label_map.pbtxt', s)\n",
        "  f.write(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h27L2wAaxois",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Here start the training, It took 3-4 hours of training the model. we can train upto 20000 steps but in this case we don't need the such step.\n",
        "## We used 5545 steps for the training purpose.\n",
        "!python /content/models/research/object_detection/legacy/train.py \\\n",
        "    --logtostderr \\\n",
        "    --train_dir=/content/models/research/object_detection/training/trained \\\n",
        "    --pipeline_config_path=/content/models/research/object_detection/training/ssd_mobilenet_v1_coco.config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3Zl4RvxyIOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Same for evaluating the model.\n",
        "!python3 /content/models/research/object_detection/legacy/eval.py \\\n",
        "    --logtostderr \\\n",
        "    --checkpoint_dir=/content/models/research/object_detection/training/trained \\\n",
        "    --pipeline_config_path=/content/models/research/object_detection/training/ssd_mobilenet_v1_coco.config \\\n",
        "    --eval_dir=/content/models/research/object_detection/training/valid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nzp28yhyvbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## here we create our model which will detect the object in image. The ouput of this cell create frozen_inference_graph.pb which is our model.\n",
        "!python /content/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type image_tensor \\\n",
        "    --pipeline_config_path /content/models/research/object_detection/training/ssd_mobilenet_v1_coco.config \\\n",
        "    --trained_checkpoint_prefix /content/models/research/object_detection/training/trained/model.ckpt-5545 \\\n",
        "    --output_directory /content/output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-NGBBEII9OW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Path\n",
        "PATH_TO_MODEL = '/content/output/frozen_inference_graph.pb'\n",
        "PATH_TO_LABELS = '/content/models/research/object_detection/training/label_map.pbtxt'\n",
        "NUM_CLASSES =1\n",
        "PATH_TO_IMAGES = '/content/data/images/ShelfImages/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSRS0hmHJOEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load frozen graph\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_MODEL, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pelg_MXTJQOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load categories (we have only 1 category pack)\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0zVQVHzJR7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's write function that executes detection\n",
        "def run_inference_for_single_image(image, image_tensor, sess, tensor_dict):\n",
        "    # Run inference\n",
        "    expanded_dims = np.expand_dims(image, 0)\n",
        "    output_dict = sess.run(tensor_dict, feed_dict={image_tensor: expanded_dims})\n",
        "    # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "    output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
        "    output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint8)\n",
        "    output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "    output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "    return output_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6mnb3BdJc-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# it is useful to be able to run inference not only on the whole image,\n",
        "# but also on its parts\n",
        "# cutoff - minimum detection scrore needed to take box\n",
        "def run_inference_for_image_part(image_tensor, sess, tensor_dict, \n",
        "                                 image, cutoff, ax0, ay0, ax1, ay1):\n",
        "    boxes = []\n",
        "    im = image[ay0:ay1, ax0:ax1]\n",
        "    h, w, c = im.shape\n",
        "    output_dict = run_inference_for_single_image(im, image_tensor, sess, tensor_dict)\n",
        "    for i in range(100):\n",
        "        if output_dict['detection_scores'][i] < cutoff:\n",
        "            break\n",
        "        y0, x0, y1, x1, score = *output_dict['detection_boxes'][i], \\\n",
        "                                output_dict['detection_scores'][i]\n",
        "        x0, y0, x1, y1, score = int(x0*w), int(y0*h), \\\n",
        "                                int(x1*w), int(y1*h), \\\n",
        "                                int(score * 100)\n",
        "        boxes.append((x0+ax0, y0+ay0, x1+ax0, y1+ay0, score))\n",
        "    return boxes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV3LODMTJfk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# additional helper function to work not with coordinates but with percents\n",
        "def run_inference_for_image_part_pcnt(image_tensor, sess, tensor_dict, \n",
        "                                 image, cutoff, p_ax0, p_ay0, p_ax1, p_ay1):\n",
        "    h, w, c = image.shape\n",
        "    max_x, max_y = w-1, h-1\n",
        "    return run_inference_for_image_part(\n",
        "                                image_tensor, sess, tensor_dict, \n",
        "                                image, cutoff, \n",
        "                                int(p_ax0*max_x), int(p_ay0*max_y), \n",
        "                                int(p_ax1*max_x), int(p_ay1*max_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2nHaycWJhyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to display image with bounding boxes\n",
        "\n",
        "org = (100, 100) \n",
        "  \n",
        "# fontScale \n",
        "fontScale = 3\n",
        "   \n",
        "# Blue color in BGR \n",
        "color = (0, 0, 255) \n",
        "  \n",
        "# Line thickness of 2 px \n",
        "thickness = 5\n",
        "\n",
        "def display_image_with_boxes(image, boxes, p_x0=0, p_y0=0, p_x1=1, p_y1=1):\n",
        "    count=0\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX \n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    for x0, y0, x1, y1, score in boxes:\n",
        "        count+=1\n",
        "        image = cv2.rectangle(image, (x0, y0), (x1, y1), (0,255,0), 5)\n",
        "    image = cv2.putText(image, f'Total Number of product: {count}', org, font,fontScale, color, thickness, cv2.LINE_AA)\n",
        "    if p_x0 != 0 or p_y0 !=0 or p_x1 != 1 or p_y1 != 1:\n",
        "        h, w, c = image.shape\n",
        "        max_x, max_y = w-1, h-1\n",
        "        image = cv2.rectangle(image, \n",
        "                              (int(p_x0*max_x), int(p_y0*max_y)), \n",
        "                              (int(p_x1*max_x), int(p_y1*max_y)), (0,0,255), 5)\n",
        "        \n",
        "    \n",
        "    plt.figure(figsize=(14, 14))\n",
        "    return count, plt.imshow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoEoWO_0JkGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_graph():\n",
        "    ops = tf.get_default_graph().get_operations()\n",
        "    all_tensor_names = {output.name\n",
        "                        for op in ops\n",
        "                        for output in op.outputs}\n",
        "    tensor_dict = {}\n",
        "    for key in ['num_detections', 'detection_boxes',\n",
        "                'detection_scores', 'detection_classes',\n",
        "                'detection_masks']:\n",
        "        tensor_name = key + ':0'\n",
        "        if tensor_name in all_tensor_names:\n",
        "            tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
        "    image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "    return image_tensor, tensor_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wfufzsAKDlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def non_max_suppression(boxes, overlapThresh):\n",
        "    if len(boxes) == 0:\n",
        "        return np.array([]).astype(\"int\")\n",
        "\n",
        "    if boxes.dtype.kind == \"i\":\n",
        "        boxes = boxes.astype(\"float\")\n",
        "\n",
        "    \n",
        "    pick = []\n",
        "\n",
        "    x1 = boxes[:,0]\n",
        "    y1 = boxes[:,1]\n",
        "    x2 = boxes[:,2]\n",
        "    y2 = boxes[:,3]\n",
        "    sc = boxes[:,4]\n",
        " \n",
        "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "    idxs = np.argsort(sc)\n",
        " \n",
        "    while len(idxs) > 0:\n",
        "        last = len(idxs) - 1\n",
        "        i = idxs[last]\n",
        "        pick.append(i)\n",
        " \n",
        "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
        "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
        "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
        "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
        " \n",
        "        w = np.maximum(0, xx2 - xx1 + 1)\n",
        "        h = np.maximum(0, yy2 - yy1 + 1)\n",
        "    \n",
        "        #todo fix overlap-contains...\n",
        "        overlap = (w * h) / area[idxs[:last]]\n",
        "         \n",
        "        idxs = np.delete(idxs, np.concatenate(([last],\n",
        "            np.where(overlap > overlapThresh)[0])))\n",
        "    \n",
        "    return boxes[pick].astype(\"int\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbphDLkWKHXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def do_sliding_window_inference_with_nm_suppression(file, cutoff):\n",
        "    with detection_graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            image_tensor, tensor_dict = initialize_graph()\n",
        "            image = cv2.imread(f'{PATH_TO_IMAGES}{file}')\n",
        "            h, w, c = image.shape\n",
        "            boxes = run_inference_for_image_part_pcnt(\n",
        "                image_tensor, sess, tensor_dict, image, cutoff, 0, 0, 1, 1)\n",
        "            a = np.array(boxes)\n",
        "            mean_dx = int(np.mean(a[:,2]-a[:,0]))\n",
        "            mean_dy = int(np.mean(a[:,3]-a[:,1]))\n",
        "            step_x, step_y = mean_dx, mean_dy\n",
        "            window_size = 2*mean_dy\n",
        "            boxes = []\n",
        "            y0 = 0\n",
        "            while y0 < h-1:\n",
        "                x0 = 0\n",
        "                while x0 < w-1:\n",
        "                    x1, y1 = x0 + window_size, y0 + window_size\n",
        "                    boxes += run_inference_for_image_part(\n",
        "                        image_tensor, sess, tensor_dict, image, cutoff, \n",
        "                        x0, y0, x1, y1)\n",
        "                    x0 += step_y\n",
        "                y0 += step_x\n",
        "            boxes = non_max_suppression(np.array(boxes), 0.5)\n",
        "    return display_image_with_boxes(image, boxes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K393bHJ7KM0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "do_sliding_window_inference_with_nm_suppression('C3_P02_N1_S2_2.JPG', 0.5) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGhuxF3rKSZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Creating JSON file \n",
        "\n",
        "import json \n",
        "  \n",
        "# Data to be written \n",
        "img_dir = 'C3_P02_N1_S2_2.JPG'\n",
        "counter,_ = do_sliding_window_inference_with_nm_suppression(img_dir, 0.5)\n",
        "dictionary ={ \n",
        "    \"file_name\" : img_dir, \n",
        "    \"no_of_product \" : counter, \n",
        "} \n",
        "  \n",
        "# Serializing json  \n",
        "json_object = json.dumps(dictionary, indent = 4) \n",
        "  \n",
        "# Writing to sample.json \n",
        "with open(\"sample.json\", \"w\") as outfile: \n",
        "    outfile.write(json_object)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT1IhsyyRPKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}